{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from groq import Groq\n",
    "import json\n",
    "import instructor\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "\n",
    "# Initialize the client with instructor\n",
    "api_key = \"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\"\n",
    "client = Groq(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTOR BASED APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_groq(client)\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportTicket(BaseModel):\n",
    "    ticket_id: str\n",
    "    order_id: str\n",
    "    user_id: str\n",
    "    date: str\n",
    "    description: str\n",
    "    category: Literal[\"delivery\", \"payment\", \"other\"]\n",
    "    priority: Optional[Literal[\"low\", \"medium\", \"high\"]] = None\n",
    "    status: Optional[Literal[\"open\", \"closed\", \"in progress\"]] = None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import UsageTracker\n",
    "\n",
    "# Clear any completion response hooks -- prevents \n",
    "# duplicate writes to the usage tracker.\n",
    "client.clear(\"completion:response\")\n",
    "\n",
    "# Create a new tracker\n",
    "tracker = UsageTracker()\n",
    "\n",
    "# Define a custom instructor hook and update the\n",
    "# tracker when a new completion runs.\n",
    "def log_completion_kwargs(*args, **kwargs):\n",
    "    usage = args[0].usage\n",
    "    tracker.track(usage)\n",
    "\n",
    "# Assign the hook to instructor -- this will make the hook\n",
    "# run each time the server returns a chat completion to us.\n",
    "client.on(\"completion:response\", log_completion_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class Complicated(BaseModel):\n",
    "    # a must be cat, dog, or hat\n",
    "    a: Literal[\"cat\", \"dog\", \"hat\"]\n",
    "    b: int\n",
    "    c: bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a='cat' b=1 c=True\n"
     ]
    }
   ],
   "source": [
    "# Clear the tracker before we run the completion\n",
    "# so we arent' tracking multiple jobs.\n",
    "tracker.clear()\n",
    "\n",
    "# Now you can use the Pydantic model directly with response_model\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Don't give me what I want\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Write me a short essay on Dolly Parton..\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_completion_tokens=1024,\n",
    "    response_model=Complicated,  # Use response_model instead of response_format\n",
    "    max_retries=4\n",
    ")\n",
    "\n",
    "print(completion)  # This will be a User instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens:   [955]\n",
      "Output tokens:  [95]\n",
      "Total tokens:   1050\n",
      "Num retries:    1\n"
     ]
    }
   ],
   "source": [
    "print(\"Input tokens:  \", tracker.input_tokens)\n",
    "print(\"Output tokens: \", tracker.output_tokens)\n",
    "print(\"Total tokens:  \", sum(tracker.total_tokens))\n",
    "print(\"Num retries:   \", len(tracker.output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id='1232399' order_id='12312399' user_id='12322399' date='2023-01-01' description='User ordered chicken biriyani and beef, but beef was missing from the order.' category='delivery' priority='high' status='open'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now you can use the Pydantic model directly with response_model\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a support ticket assistant working in a food delivery company. You will be given a description /\n",
    "             of a support ticket raised by a user. Your task is to extract the following information from the description: /\n",
    "             \n",
    "             1. ticket_id   \n",
    "             2. order_id\n",
    "             3. user_id\n",
    "             4. date\n",
    "             5. description (create a summary of the user's ticket)\n",
    "             6. category (e.g. delivery, payment, etc.)\n",
    "             7. priority (e.g. low, medium, high)\n",
    "             8. status (e.g. open, closed, in progress)\n",
    "\n",
    "             DO NOT include any additional information.\n",
    "             \n",
    "             \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\" I ordered a chicken biriyani and beef. but beef is missing. order was placed on 2023-01-01. ticket_id is 1232399, order_id is 12312399, user_id is 12322399\"\"\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_completion_tokens=1024,\n",
    "    response_model=SupportTicket  # Use response_model instead of response_format\n",
    ")\n",
    "\n",
    "print(completion)  # This will be a User instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "# Downloads the model from HuggingFace if you don't already have it, \n",
    "# then loads it into memory\n",
    "model = outlines.models.transformers(\n",
    "    \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<outlines.generate.api.SequenceGeneratorAdapter at 0x17a943640>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from outlines_utils import track_logits\n",
    "\n",
    "generator = outlines.generate.json(\n",
    "    model, \n",
    "    User,\n",
    "    sampler = outlines.samplers.greedy()\n",
    ")\n",
    "\n",
    "# Add tools to track token probabilities as they are generated\n",
    "track_logits(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You create users.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a user with a name and an age.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from outlines_utils import template\n",
    "\n",
    "print(template(\n",
    "    model, \n",
    "    \"Give me a user with a name and an age.\",\n",
    "    system_prompt=\"You create users.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/genai/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/genai/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "User(name='John', age=30, email=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any previously tracked logits\n",
    "generator.logits_processor.clear()\n",
    "\n",
    "person = generator(\n",
    "    template(\n",
    "        model, \n",
    "        \"Give me a person with a name and an age.\",\n",
    "        system_prompt=\"You create users.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
