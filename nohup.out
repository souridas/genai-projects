INFO 04-27 23:53:20 [__init__.py:239] Automatically detected platform cpu.
INFO 04-27 23:53:20 [api_server.py:1034] vLLM API server version 0.8.3
INFO 04-27 23:53:20 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='TinyLlama/TinyLlama-1.1B-Chat-v1.0', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='souridas510', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x13a0f9260>)
INFO 04-27 23:53:22 [config.py:2673] For macOS with Apple Silicon, currently bfloat16 is not supported. Setting dtype to float16.
WARNING 04-27 23:53:22 [config.py:2704] Casting torch.bfloat16 to torch.float16.
INFO 04-27 23:53:25 [config.py:600] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.
WARNING 04-27 23:53:25 [arg_utils.py:1708] device type=cpu is not supported by the V1 Engine. Falling back to V0. 
INFO 04-27 23:53:25 [config.py:1634] Disabled the custom all-reduce kernel because it is not supported on current platform.
WARNING 04-27 23:53:25 [cpu.py:106] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.
WARNING 04-27 23:53:25 [cpu.py:119] uni is not supported on CPU, fallback to mp distributed executor backend.
INFO 04-27 23:53:25 [api_server.py:246] Started engine process with PID 75617
INFO 04-27 23:53:26 [__init__.py:239] Automatically detected platform cpu.
INFO 04-27 23:53:27 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 04-27 23:53:28 [cpu.py:45] Using Torch SDPA backend.
INFO 04-27 23:53:28 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 04-27 23:53:28 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 04-27 23:53:29 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 04-27 23:53:29 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
INFO 04-27 23:53:32 [__init__.py:239] Automatically detected platform cpu.
INFO 04-27 23:53:33 [api_server.py:1034] vLLM API server version 0.8.3
INFO 04-27 23:53:33 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='TinyLlama/TinyLlama-1.1B-Chat-v1.0', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='souridas510', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x131afd260>)
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.84s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.84s/it]

INFO 04-27 23:53:33 [loader.py:447] Loading weights took 3.85 seconds
INFO 04-27 23:53:33 [executor_base.py:112] # cpu blocks: 11915, # CPU blocks: 0
INFO 04-27 23:53:33 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 93.09x
INFO 04-27 23:53:34 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 0.59 seconds
INFO 04-27 23:53:34 [config.py:2673] For macOS with Apple Silicon, currently bfloat16 is not supported. Setting dtype to float16.
WARNING 04-27 23:53:34 [config.py:2704] Casting torch.bfloat16 to torch.float16.
INFO 04-27 23:53:34 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000
INFO 04-27 23:53:34 [launcher.py:26] Available routes are:
INFO 04-27 23:53:34 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD
INFO 04-27 23:53:34 [launcher.py:34] Route: /docs, Methods: GET, HEAD
INFO 04-27 23:53:34 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 04-27 23:53:34 [launcher.py:34] Route: /redoc, Methods: GET, HEAD
INFO 04-27 23:53:34 [launcher.py:34] Route: /health, Methods: GET
INFO 04-27 23:53:34 [launcher.py:34] Route: /load, Methods: GET
INFO 04-27 23:53:34 [launcher.py:34] Route: /ping, Methods: GET, POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 04-27 23:53:34 [launcher.py:34] Route: /version, Methods: GET
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /pooling, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /score, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /rerank, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /invocations, Methods: POST
INFO 04-27 23:53:34 [launcher.py:34] Route: /metrics, Methods: GET
INFO:     Started server process [75594]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 04-27 23:53:37 [config.py:600] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.
WARNING 04-27 23:53:37 [arg_utils.py:1708] device type=cpu is not supported by the V1 Engine. Falling back to V0. 
INFO 04-27 23:53:37 [config.py:1634] Disabled the custom all-reduce kernel because it is not supported on current platform.
WARNING 04-27 23:53:37 [cpu.py:106] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.
WARNING 04-27 23:53:37 [cpu.py:119] uni is not supported on CPU, fallback to mp distributed executor backend.
INFO 04-27 23:53:37 [api_server.py:246] Started engine process with PID 75646
INFO 04-27 23:53:39 [__init__.py:239] Automatically detected platform cpu.
INFO 04-27 23:53:39 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', speculative_config=None, tokenizer='TinyLlama/TinyLlama-1.1B-Chat-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=TinyLlama/TinyLlama-1.1B-Chat-v1.0, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 04-27 23:53:40 [cpu.py:45] Using Torch SDPA backend.
INFO 04-27 23:53:40 [importing.py:16] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 04-27 23:53:40 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 04-27 23:53:41 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 04-27 23:53:41 [weight_utils.py:315] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.34s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.34s/it]

INFO 04-27 23:53:46 [loader.py:447] Loading weights took 4.35 seconds
INFO 04-27 23:53:46 [executor_base.py:112] # cpu blocks: 11915, # CPU blocks: 0
INFO 04-27 23:53:46 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 93.09x
INFO 04-27 23:53:46 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 0.45 seconds
INFO 04-27 23:53:47 [api_server.py:1081] Starting vLLM API server on http://0.0.0.0:8000
INFO 04-27 23:53:47 [launcher.py:26] Available routes are:
INFO 04-27 23:53:47 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD
INFO 04-27 23:53:47 [launcher.py:34] Route: /docs, Methods: GET, HEAD
INFO 04-27 23:53:47 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 04-27 23:53:47 [launcher.py:34] Route: /redoc, Methods: GET, HEAD
INFO 04-27 23:53:47 [launcher.py:34] Route: /health, Methods: GET
INFO 04-27 23:53:47 [launcher.py:34] Route: /load, Methods: GET
INFO 04-27 23:53:47 [launcher.py:34] Route: /ping, Methods: GET, POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /tokenize, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /detokenize, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/models, Methods: GET
INFO 04-27 23:53:47 [launcher.py:34] Route: /version, Methods: GET
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/chat/completions, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/completions, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/embeddings, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /pooling, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /score, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/score, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /rerank, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v1/rerank, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /v2/rerank, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /invocations, Methods: POST
INFO 04-27 23:53:47 [launcher.py:34] Route: /metrics, Methods: GET
INFO:     Started server process [75627]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 04-27 23:54:05 [chat_utils.py:396] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 04-27 23:54:05 [logger.py:39] Received request chatcmpl-de0f7717c66a47e38341673d44274619: prompt: '<|user|>\nHello, how are you?</s>\n<|assistant|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2026, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-27 23:54:05 [engine.py:310] Added request chatcmpl-de0f7717c66a47e38341673d44274619.
WARNING 04-27 23:54:06 [cpu.py:170] Pin memory is not supported on CPU.
INFO:     127.0.0.1:58751 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-27 23:54:17 [metrics.py:488] Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 2.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-27 23:54:27 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-27 23:56:26 [logger.py:39] Received request chatcmpl-f5d70cddee8e4fce9b3e696f3080e7d6: prompt: '<|user|>\nink pinky ponky ...</s>\n<|assistant|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2025, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 04-27 23:56:26 [engine.py:310] Added request chatcmpl-f5d70cddee8e4fce9b3e696f3080e7d6.
INFO 04-27 23:56:27 [metrics.py:488] Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-27 23:56:32 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:58985 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-27 23:56:42 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-27 23:56:52 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-27 23:57:17 [logger.py:39] Received request chatcmpl-4ac8275718564ab4a042485eda157aef: prompt: '<|user|>\ncreate a random json object.</s>\n<|assistant|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2026, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json=None, regex=None, choice=None, grammar=None, json_object=True, backend=None, whitespace_pattern=None), extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 04-27 23:57:17 [__init__.py:33] xgrammar module cannot be imported successfully. Falling back to use outlines instead.
WARNING 04-27 23:57:17 [__init__.py:33] outlines does not support json_object. Falling back to use guidance instead.
INFO 04-27 23:57:18 [engine.py:310] Added request chatcmpl-4ac8275718564ab4a042485eda157aef.
INFO 04-27 23:57:23 [metrics.py:488] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:59157 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-27 23:57:34 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-27 23:57:44 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-28 00:12:26 [logger.py:39] Received request chatcmpl-b367da526414414eb00fbaf83392a603: prompt: '<|user|>\ncreate a user.</s>\n<|assistant|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2028, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'hobbies': {'items': {'type': 'string'}, 'title': 'Hobbies', 'type': 'array'}, 'job': {'title': 'Job', 'type': 'string'}}, 'required': ['age', 'hobbies', 'job', 'name'], 'type': 'object'}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None), extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 04-28 00:12:26 [__init__.py:33] xgrammar module cannot be imported successfully. Falling back to use outlines instead.
INFO 04-28 00:12:27 [engine.py:310] Added request chatcmpl-b367da526414414eb00fbaf83392a603.
INFO:     127.0.0.1:61127 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-28 00:12:39 [metrics.py:488] Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 3.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-28 00:12:42 [logger.py:39] Received request chatcmpl-39a180af71494623a782f391171448d7: prompt: '<|user|>\ncreate a user.</s>\n<|assistant|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2028, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=GuidedDecodingParams(json={'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}, 'hobbies': {'items': {'type': 'string'}, 'title': 'Hobbies', 'type': 'array'}, 'job': {'title': 'Job', 'type': 'string'}}, 'required': ['age', 'hobbies', 'job', 'name'], 'type': 'object'}, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None), extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 04-28 00:12:42 [__init__.py:33] xgrammar module cannot be imported successfully. Falling back to use outlines instead.
INFO 04-28 00:12:42 [engine.py:310] Added request chatcmpl-39a180af71494623a782f391171448d7.
INFO:     127.0.0.1:61168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 04-28 00:12:54 [metrics.py:488] Avg prompt throughput: 1.4 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-28 00:13:04 [metrics.py:488] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 04-28 00:22:54 [launcher.py:74] Shutting down FastAPI HTTP server.
Exception ignored in: <function Socket.__del__ at 0x1384cf9c0>
Traceback (most recent call last):
  File "/Users/souria/tgi-venv/lib/python3.12/site-packages/zmq/sugar/socket.py", line 184, in __del__
    def __del__(self):

  File "/Users/souria/tgi-venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 426, in signal_handler
    raise KeyboardInterrupt("MQLLMEngine terminated")
KeyboardInterrupt: MQLLMEngine terminated
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
